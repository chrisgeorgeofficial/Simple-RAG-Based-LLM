{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfba32f",
   "metadata": {},
   "source": [
    "### Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8600d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6476a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.pdf', 'author': 'John Doe', 'page': 1, 'date_created': '2025-01-01'}, page_content='This is the content of the document.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(page_content=\"This is the content of the document.\", metadata={\"source\": \"example.pdf\",\n",
    "                                                                            \"author\": \"John Doe\",\n",
    "                                                                            \"page\": 1,\n",
    "                                                                            \"date_created\": \"2025-01-01\"})\n",
    "#We apply filters using the metadata\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19799df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b540ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created.\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\": \"One of Python’s greatest strengths lies in its versatility. It is widely used across different fields such as web development, data science, artificial intelligence, machine learning, automation, and game development. Frameworks like Django and Flask make web development faster, while libraries such as NumPy, Pandas, and TensorFlow empower developers to work with data analysis, AI, and deep learning. This broad ecosystem of libraries and frameworks makes Python adaptable for almost any type of project.\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c02fef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='One of Python’s greatest strengths lies in its versatility. It is widely used across different fields such as web development, data science, artificial intelligence, machine learning, automation, and game development. Frameworks like Django and Flask make web development faster, while libraries such as NumPy, Pandas, and TensorFlow empower developers to work with data analysis, AI, and deep learning. This broad ecosystem of libraries and frameworks makes Python adaptable for almost any type of project.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()  # Returns a list of Document objects\n",
    "print(document)  # Display the loaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2225d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro-sec1.txt'}, page_content='One of Python’s greatest strengths lies in its versatility. It is widely used across different fields such as web development, data science, artificial intelligence, machine learning, automation, and game development. Frameworks like Django and Flask make web development faster, while libraries such as NumPy, Pandas, and TensorFlow empower developers to work with data analysis, AI, and deep learning. This broad ecosystem of libraries and frameworks makes Python adaptable for almost any type of project.'), Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='One of Python’s greatest strengths lies in its versatility. It is widely used across different fields such as web development, data science, artificial intelligence, machine learning, automation, and game development. Frameworks like Django and Flask make web development faster, while libraries such as NumPy, Pandas, and TensorFlow empower developers to work with data analysis, AI, and deep learning. This broad ecosystem of libraries and frameworks makes Python adaptable for almost any type of project.')]\n"
     ]
    }
   ],
   "source": [
    "### Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"../data/text_files/\", glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"},show_progress=False)\n",
    "documents = loader.load()  # Returns a list of Document objects\n",
    "print(documents)  # Display the loaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa472a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content=\"CHRIS GEORGE\\nchrisgeorge2k20@gmail.com\\nChalakudy,Thrissur,Kerala\\n@chrisgeorgeofficial\\n7907703013\\n@chrisgeorgeofficial\\nProfile\\nDedicated Computer Science Engineer with a strong foundation in Python, SQL, AI Frameworks, and Machine \\nLearning. Internships have provided valuable experience in developing web applications and data-driven solutions. \\nProven ability to apply knowledge to solve real-world problems. Actively seeking opportunities to learn, grow, and \\ncontribute to a dynamic team.\\nEducation\\n2021 – 2025\\nCherthala\\nBachelor of Technology in Computer Science Engineering\\nCollege Of Engineering , Cherthala\\nCGPA : 7.97\\n2020 – 2021\\nHigher Secondary Education\\nSt.Antony's Higher Secondary School, Mala\\nPERCENTAGE : 96.25%\\n2019 – 2020\\nSecondary Education\\nSt.Joseph's E.M.H.S.S Schools Aloor\\nPERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms \\nand provides personalized health advice Using ANN and Mistral-7B\\n2024\\nStock Sentiment Analysis using Machine Learning\\nDeveloped a machine learning model to predict stock market sentiment based on \\nfinancial news headlines Using Random Forest.\\n2024\\nDoctor Appointment Booking Website in React JS\\nBuilt a Frontend for Appointment Booking Website using React JS with clean and \\nmodern UI design.\\n2024\\nMachine Learning - Based Human Disease Diagnosis System\\nDesigned a Predictive disease identification system using Machine Learning, \\ndemonstrating expertise in data analysis, algorithm development, and model evaluation.\\n2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023\\nGame Shopping and Merchandise Frontend Project\\nDeveloped a game shopping and merchandise platform, showcasing skills in ecommerce \\ndevelopment, user interface design .Utilized technologies and tools to create a seamless \\nuser experience, demonstrating proficiency in Frontend development using Html, Css, \\nJavascript & Bootsrap\"), Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 1}, page_content='Professional Experience\\n2025 – present\\nAI Intern\\nBOTANGELOS - AI & Automation\\nPython, AI, Microsoft Azure, Azure AI Foundry, Copilot Studio, Mysql, Sql, Api \\nDevelopment, Ollama, Frontend,Desktop applications, Automation, Qwen models, gpt \\nmodels, LLaMa\\n2024\\nInternship On Data Science\\nAcademy of skill development and Ardent Software\\nCompleted a 4 week internship and industrial training focused on machine learning, AI, \\nand data science, culminating in the development of a stock sentiment analysis project.\\n2023\\nInternship On React.js, Express.js,Node.js and Mysql\\nDevfactory\\nCompleted a 10-day internship focused on developing a matrimony website using \\nReact.js, Express.js, Node.js, and MySQL.\\nSkills\\nProgramming Languages\\nPython, C, SQL, HTML, CSS\\nSoft Skills\\nTeamwork, Listening, Fast learning\\nTools\\nVisual Studio Code, Google Collab, Jupyter-Notebook\\nFrameworks (Currently Working)\\nLangChain, LangGraph, A2A Protocol, Mcp, Crew Ai \\nCourses\\n2024\\nPython for Data Science\\nNPTEL-IIT Madras\\nCompleted the \"Python for Data Science\" MOOC course from NPTEL, gaining \\nproficiency in Python programming for data analysis, data visualization, and machine \\nlearning.\\n2024\\nMachine Learning for Engineering and Science Applications\\nNPTEL-IIT Madras\\nGained Knowledge about ML algorithms like Decision Trees, SVM, and Neural \\nNetworks.\\nAwards\\n2025\\nFirst Prize - Project Expo (Software) | IHRD Tharang 2K25\\nIHRD Tharang 2k25 - National Level Techno Cultural Fest\\nAwarded 1st place at IHRD Tharang 2K25, a national-level techno-cultural fest, for \\ndeveloping \"Where is My Bus?\", a real-time bus tracking software. The system allows \\nusers to enter departure and destination points to view available buses and track their \\nlive locations, enhancing convenience for commuters.')]\n"
     ]
    }
   ],
   "source": [
    "### Directory Loader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(\"../data/pdf/\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=False)\n",
    "pdf_documents = loader.load()  # Returns a list of Document objects\n",
    "print(pdf_documents)  # Display the loaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af8d7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "944d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],length_function=len\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    # show example of a chunk\n",
    "\n",
    "    if(split_docs):\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")  # Print first 200 characters of the first chunk\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae75c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 2 documents into 5 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: CHRIS GEORGE\n",
      "chrisgeorge2k20@gmail.com\n",
      "Chalakudy,Thrissur,Kerala\n",
      "@chrisgeorgeofficial\n",
      "7907703013\n",
      "@chrisgeorgeofficial\n",
      "Profile\n",
      "Dedicated Computer Science Engineer with a strong foundation in Python, SQ...\n",
      "Metadata: {'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content=\"CHRIS GEORGE\\nchrisgeorge2k20@gmail.com\\nChalakudy,Thrissur,Kerala\\n@chrisgeorgeofficial\\n7907703013\\n@chrisgeorgeofficial\\nProfile\\nDedicated Computer Science Engineer with a strong foundation in Python, SQL, AI Frameworks, and Machine \\nLearning. Internships have provided valuable experience in developing web applications and data-driven solutions. \\nProven ability to apply knowledge to solve real-world problems. Actively seeking opportunities to learn, grow, and \\ncontribute to a dynamic team.\\nEducation\\n2021 – 2025\\nCherthala\\nBachelor of Technology in Computer Science Engineering\\nCollege Of Engineering , Cherthala\\nCGPA : 7.97\\n2020 – 2021\\nHigher Secondary Education\\nSt.Antony's Higher Secondary School, Mala\\nPERCENTAGE : 96.25%\\n2019 – 2020\\nSecondary Education\\nSt.Joseph's E.M.H.S.S Schools Aloor\\nPERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content='PERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms \\nand provides personalized health advice Using ANN and Mistral-7B\\n2024\\nStock Sentiment Analysis using Machine Learning\\nDeveloped a machine learning model to predict stock market sentiment based on \\nfinancial news headlines Using Random Forest.\\n2024\\nDoctor Appointment Booking Website in React JS\\nBuilt a Frontend for Appointment Booking Website using React JS with clean and \\nmodern UI design.\\n2024\\nMachine Learning - Based Human Disease Diagnosis System\\nDesigned a Predictive disease identification system using Machine Learning, \\ndemonstrating expertise in data analysis, algorithm development, and model evaluation.\\n2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content='2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023\\nGame Shopping and Merchandise Frontend Project\\nDeveloped a game shopping and merchandise platform, showcasing skills in ecommerce \\ndevelopment, user interface design .Utilized technologies and tools to create a seamless \\nuser experience, demonstrating proficiency in Frontend development using Html, Css, \\nJavascript & Bootsrap'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 1}, page_content='Professional Experience\\n2025 – present\\nAI Intern\\nBOTANGELOS - AI & Automation\\nPython, AI, Microsoft Azure, Azure AI Foundry, Copilot Studio, Mysql, Sql, Api \\nDevelopment, Ollama, Frontend,Desktop applications, Automation, Qwen models, gpt \\nmodels, LLaMa\\n2024\\nInternship On Data Science\\nAcademy of skill development and Ardent Software\\nCompleted a 4 week internship and industrial training focused on machine learning, AI, \\nand data science, culminating in the development of a stock sentiment analysis project.\\n2023\\nInternship On React.js, Express.js,Node.js and Mysql\\nDevfactory\\nCompleted a 10-day internship focused on developing a matrimony website using \\nReact.js, Express.js, Node.js, and MySQL.\\nSkills\\nProgramming Languages\\nPython, C, SQL, HTML, CSS\\nSoft Skills\\nTeamwork, Listening, Fast learning\\nTools\\nVisual Studio Code, Google Collab, Jupyter-Notebook\\nFrameworks (Currently Working)\\nLangChain, LangGraph, A2A Protocol, Mcp, Crew Ai \\nCourses\\n2024\\nPython for Data Science\\nNPTEL-IIT Madras'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 1}, page_content='Tools\\nVisual Studio Code, Google Collab, Jupyter-Notebook\\nFrameworks (Currently Working)\\nLangChain, LangGraph, A2A Protocol, Mcp, Crew Ai \\nCourses\\n2024\\nPython for Data Science\\nNPTEL-IIT Madras\\nCompleted the \"Python for Data Science\" MOOC course from NPTEL, gaining \\nproficiency in Python programming for data analysis, data visualization, and machine \\nlearning.\\n2024\\nMachine Learning for Engineering and Science Applications\\nNPTEL-IIT Madras\\nGained Knowledge about ML algorithms like Decision Trees, SVM, and Neural \\nNetworks.\\nAwards\\n2025\\nFirst Prize - Project Expo (Software) | IHRD Tharang 2K25\\nIHRD Tharang 2k25 - National Level Techno Cultural Fest\\nAwarded 1st place at IHRD Tharang 2K25, a national-level techno-cultural fest, for \\ndeveloping \"Where is My Bus?\", a real-time bus tracking software. The system allows \\nusers to enter departure and destination points to view available buses and track their \\nlive locations, enhancing convenience for commuters.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2559304",
   "metadata": {},
   "source": [
    "### Embedding And Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d63bc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d40477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x24306139150>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embeddings using SentenceTransformers\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Intialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The Hugging face model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): The input texts to generate embeddings for\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The generated embeddings as a numpy array\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded\")\n",
    "\n",
    "        print(f\"Generating embedding for {len(texts)} texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings of shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    ## Initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6626d17",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caed2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection 'pdf_documents' initialized at '../data/vector_store'\n",
      "Current number of documents in collection: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x24306116050>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromaDB vector store\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name (str): The name of the collection in chromaDB\n",
    "            persist_directory (str): The directory to persist the chromaDB data\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize the chromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(name=self.collection_name,metadata={\"description\": \"PDF Document embeddings for RAG\"})\n",
    "            print(f\"ChromaDB collection '{self.collection_name}' initialized at '{self.persist_directory}'\")\n",
    "            print(f\"Current number of documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing chromaDB: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Any]): The list of Document objects\n",
    "            embeddings (np.ndarray): The corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store\")\n",
    "        \n",
    "        #prepare data for chromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate a unique ID \n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #prepare metadata\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['context_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "            # Add to collection\n",
    "\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    metadatas=metadatas,\n",
    "                    documents=documents_text,\n",
    "                    embeddings=embeddings_list\n",
    "                )\n",
    "                print(f\"Successfully added {len(documents)} documents to the vector store\")\n",
    "                print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding documents to chromaDB: {e}\")\n",
    "                raise\n",
    "\n",
    "VectorStore=VectorStore()\n",
    "VectorStore\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05546ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content=\"CHRIS GEORGE\\nchrisgeorge2k20@gmail.com\\nChalakudy,Thrissur,Kerala\\n@chrisgeorgeofficial\\n7907703013\\n@chrisgeorgeofficial\\nProfile\\nDedicated Computer Science Engineer with a strong foundation in Python, SQL, AI Frameworks, and Machine \\nLearning. Internships have provided valuable experience in developing web applications and data-driven solutions. \\nProven ability to apply knowledge to solve real-world problems. Actively seeking opportunities to learn, grow, and \\ncontribute to a dynamic team.\\nEducation\\n2021 – 2025\\nCherthala\\nBachelor of Technology in Computer Science Engineering\\nCollege Of Engineering , Cherthala\\nCGPA : 7.97\\n2020 – 2021\\nHigher Secondary Education\\nSt.Antony's Higher Secondary School, Mala\\nPERCENTAGE : 96.25%\\n2019 – 2020\\nSecondary Education\\nSt.Joseph's E.M.H.S.S Schools Aloor\\nPERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content='PERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms \\nand provides personalized health advice Using ANN and Mistral-7B\\n2024\\nStock Sentiment Analysis using Machine Learning\\nDeveloped a machine learning model to predict stock market sentiment based on \\nfinancial news headlines Using Random Forest.\\n2024\\nDoctor Appointment Booking Website in React JS\\nBuilt a Frontend for Appointment Booking Website using React JS with clean and \\nmodern UI design.\\n2024\\nMachine Learning - Based Human Disease Diagnosis System\\nDesigned a Predictive disease identification system using Machine Learning, \\ndemonstrating expertise in data analysis, algorithm development, and model evaluation.\\n2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 0}, page_content='2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023\\nGame Shopping and Merchandise Frontend Project\\nDeveloped a game shopping and merchandise platform, showcasing skills in ecommerce \\ndevelopment, user interface design .Utilized technologies and tools to create a seamless \\nuser experience, demonstrating proficiency in Frontend development using Html, Css, \\nJavascript & Bootsrap'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 1}, page_content='Professional Experience\\n2025 – present\\nAI Intern\\nBOTANGELOS - AI & Automation\\nPython, AI, Microsoft Azure, Azure AI Foundry, Copilot Studio, Mysql, Sql, Api \\nDevelopment, Ollama, Frontend,Desktop applications, Automation, Qwen models, gpt \\nmodels, LLaMa\\n2024\\nInternship On Data Science\\nAcademy of skill development and Ardent Software\\nCompleted a 4 week internship and industrial training focused on machine learning, AI, \\nand data science, culminating in the development of a stock sentiment analysis project.\\n2023\\nInternship On React.js, Express.js,Node.js and Mysql\\nDevfactory\\nCompleted a 10-day internship focused on developing a matrimony website using \\nReact.js, Express.js, Node.js, and MySQL.\\nSkills\\nProgramming Languages\\nPython, C, SQL, HTML, CSS\\nSoft Skills\\nTeamwork, Listening, Fast learning\\nTools\\nVisual Studio Code, Google Collab, Jupyter-Notebook\\nFrameworks (Currently Working)\\nLangChain, LangGraph, A2A Protocol, Mcp, Crew Ai \\nCourses\\n2024\\nPython for Data Science\\nNPTEL-IIT Madras'),\n",
       " Document(metadata={'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'moddate': '2025-09-01T15:58:49+00:00', 'trapped': '', 'modDate': \"D:20250901155849+00'00'\", 'creationDate': \"D:20250901155849+00'00'\", 'page': 1}, page_content='Tools\\nVisual Studio Code, Google Collab, Jupyter-Notebook\\nFrameworks (Currently Working)\\nLangChain, LangGraph, A2A Protocol, Mcp, Crew Ai \\nCourses\\n2024\\nPython for Data Science\\nNPTEL-IIT Madras\\nCompleted the \"Python for Data Science\" MOOC course from NPTEL, gaining \\nproficiency in Python programming for data analysis, data visualization, and machine \\nlearning.\\n2024\\nMachine Learning for Engineering and Science Applications\\nNPTEL-IIT Madras\\nGained Knowledge about ML algorithms like Decision Trees, SVM, and Neural \\nNetworks.\\nAwards\\n2025\\nFirst Prize - Project Expo (Software) | IHRD Tharang 2K25\\nIHRD Tharang 2k25 - National Level Techno Cultural Fest\\nAwarded 1st place at IHRD Tharang 2K25, a national-level techno-cultural fest, for \\ndeveloping \"Where is My Bus?\", a real-time bus tracking software. The system allows \\nusers to enter departure and destination points to view available buses and track their \\nlive locations, enhancing convenience for commuters.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1570dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for 5 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (5, 384)\n",
      "Adding 5 documents to the vector store\n",
      "Successfully added 5 documents to the vector store\n",
      "Total documents in collection: 14\n",
      "Successfully added 5 documents to the vector store\n",
      "Total documents in collection: 15\n",
      "Successfully added 5 documents to the vector store\n",
      "Total documents in collection: 16\n",
      "Successfully added 5 documents to the vector store\n",
      "Total documents in collection: 17\n",
      "Successfully added 5 documents to the vector store\n",
      "Total documents in collection: 18\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate embeddings\n",
    "embeddings = embedding_manager.generate_embedding(texts)\n",
    "\n",
    "## Store in vector database\n",
    "VectorStore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcaa72c",
   "metadata": {},
   "source": [
    "## Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    def __init__(self, embedding_manager: EmbeddingManager, vector_store: VectorStore):\n",
    "        \"\"\"Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            embedding_manager (EmbeddingManager): The embedding manager instance\n",
    "            vector_store (VectorStore): The vector store instance\n",
    "        \"\"\"\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def retrieve(self, query:str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents for a given query\n",
    "        \n",
    "        Args:\n",
    "            query (str): The input query string\n",
    "            top_k (int): The number of top similar documents to retrieve\n",
    "            score_threshold (float): The minimum score threshold for a document to be considered relevant\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of relevant documents and their metadata\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.embedding_manager.generate_embedding([query])[0]\n",
    "\n",
    "        # Search in the vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document,metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity_score = 1 - distance  # Convert distance to similarity score\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"score\": similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                        print(f\"Retrieved doc {i}: ID={doc_id}, Score={similarity_score:.4f}, Metadata={metadata}, {len(retrieved_docs)} docs found so far\")\n",
    "                    else:\n",
    "                        print(f\"Doc {i} with ID={doc_id} filtered out due to low score: {similarity_score:.4f}\")\n",
    "                        print(\"No documents found above the score threshold.\")\n",
    "                    return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while retrieving documents: {e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriever = RAGRetriever(embedding_manager, VectorStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3350c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x24306262f50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c51d092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: ' Healthcare Chatbot for Disease Diagnosis and Guidance'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved doc 0: ID=doc_23ced2ec_1, Score=0.1234, Metadata={'title': '', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'trapped': '', 'producer': 'Skia/PDF m135', 'doc_index': 1, 'creationdate': '2025-09-01T15:58:49+00:00', 'format': 'PDF 1.4', 'total_pages': 2, 'page': 0, 'context_length': 992, 'creationDate': \"D:20250901155849+00'00'\", 'author': '', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'modDate': \"D:20250901155849+00'00'\", 'moddate': '2025-09-01T15:58:49+00:00', 'subject': '', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'creator': 'FlowCV - https://flowcv.com'}, 1 docs found so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_23ced2ec_1',\n",
       "  'content': 'PERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms \\nand provides personalized health advice Using ANN and Mistral-7B\\n2024\\nStock Sentiment Analysis using Machine Learning\\nDeveloped a machine learning model to predict stock market sentiment based on \\nfinancial news headlines Using Random Forest.\\n2024\\nDoctor Appointment Booking Website in React JS\\nBuilt a Frontend for Appointment Booking Website using React JS with clean and \\nmodern UI design.\\n2024\\nMachine Learning - Based Human Disease Diagnosis System\\nDesigned a Predictive disease identification system using Machine Learning, \\ndemonstrating expertise in data analysis, algorithm development, and model evaluation.\\n2024\\nPersonal Portfolio Website Development\\nDesigned and developed a responsive personal portfolio website using HTML, CSS, and \\nJavaScript to showcase projects, skills, and contact information.\\n2023',\n",
       "  'metadata': {'title': '',\n",
       "   'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf',\n",
       "   'trapped': '',\n",
       "   'producer': 'Skia/PDF m135',\n",
       "   'doc_index': 1,\n",
       "   'creationdate': '2025-09-01T15:58:49+00:00',\n",
       "   'format': 'PDF 1.4',\n",
       "   'total_pages': 2,\n",
       "   'page': 0,\n",
       "   'context_length': 992,\n",
       "   'creationDate': \"D:20250901155849+00'00'\",\n",
       "   'author': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf',\n",
       "   'modDate': \"D:20250901155849+00'00'\",\n",
       "   'moddate': '2025-09-01T15:58:49+00:00',\n",
       "   'subject': '',\n",
       "   'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com',\n",
       "   'creator': 'FlowCV - https://flowcv.com'},\n",
       "  'score': 0.12344402074813843,\n",
       "  'distance': 0.8765559792518616,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\" Healthcare Chatbot for Disease Diagnosis and Guidance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccd54c",
   "metadata": {},
   "source": [
    "## Integration Vectordb Context Pipeline With LLM output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1e17a",
   "metadata": {},
   "source": [
    "### Simple RAG pipeline with Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef81d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Gemini LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyDXu3k_8-24DFga_gZkr1rGNmyLQYxOsH0\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_retries=2)\n",
    "\n",
    "## 2. Simple RAG function : retrieve context + generate response\n",
    "\n",
    "def rag_simple(query,retriever, llm, top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answer using Gemini LLM\n",
    "\n",
    "    prompt=f\"\"\" Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "        Question {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    response = llm.invoke(prompt.format(context=context,query=query))\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7076ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved doc 0: ID=doc_23ced2ec_1, Score=0.1606, Metadata={'subject': '', 'format': 'PDF 1.4', 'title': '', 'author': '', 'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'creationdate': '2025-09-01T15:58:49+00:00', 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'modDate': \"D:20250901155849+00'00'\", 'doc_index': 1, 'trapped': '', 'moddate': '2025-09-01T15:58:49+00:00', 'context_length': 992, 'page': 0, 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'creationDate': \"D:20250901155849+00'00'\", 'total_pages': 2}, 1 docs found so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Year:** 2025\n",
      "**Description:** Developed an intelligent chatbot that predicts potential diseases based on user symptoms and provides personalized health advice.\n",
      "**Technologies Used:** ANN and Mistral-7B\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919d3b0",
   "metadata": {},
   "source": [
    "## Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "440055c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved doc 0: ID=doc_23ced2ec_1, Score=0.1606, Metadata={'format': 'PDF 1.4', 'creator': 'FlowCV - https://flowcv.com', 'trapped': '', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'author': '', 'page': 0, 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'total_pages': 2, 'moddate': '2025-09-01T15:58:49+00:00', 'creationDate': \"D:20250901155849+00'00'\", 'subject': '', 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'context_length': 992, 'creationdate': '2025-09-01T15:58:49+00:00', 'title': '', 'doc_index': 1, 'modDate': \"D:20250901155849+00'00'\", 'producer': 'Skia/PDF m135'}, 1 docs found so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: **Project Name:** Healthcare Chatbot for Disease Diagnosis and Guidance\n",
      "**Year:** 2025\n",
      "**Description:** Developed an intelligent chatbot that predicts potential diseases based on user symptoms and provides personalized health advice.\n",
      "**Technologies Used:** ANN and Mistral-7B\n",
      "Sources: [{'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'page': 0, 'score': 0.16064727306365967, 'preview': 'PERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot that predicts potential diseases based on user symptoms \\nand provides personalized health advice Using ANN and Mistral-7B\\n2024\\nStock Sentiment Analysis using Machine Learning\\nDevelop...'}]\n",
      "Confidence: 0.16064727306365967\n",
      "Context Preview: PERCENTAGE : 98%\n",
      "Projects\n",
      "2025\n",
      "Healthcare Chatbot for Disease Diagnosis and Guidance\n",
      "Developed an intelligent chatbot that predicts potential diseases based on user symptoms \n",
      "and provides personalized health advice Using ANN and Mistral-7B\n",
      "2024\n",
      "Stock Sentiment Analysis using Machine Learning\n",
      "Develop...\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources= [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['score'] for doc in results])\n",
    "\n",
    "    # Generate answer\n",
    "    prompt=f\"\"\" Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "        Question {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "\n",
    "    output={\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence,\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "#Example usage\n",
    "result = rag_advanced(\"Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300] + '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97848cd",
   "metadata": {},
   "source": [
    "### Advanced RAG Pipeline : Streaming, Citations, History, Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b934936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved doc 0: ID=doc_23ced2ec_1, Score=0.1606, Metadata={'title': '', 'subject': '', 'modDate': \"D:20250901155849+00'00'\", 'trapped': '', 'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'doc_index': 1, 'author': '', 'context_length': 992, 'creationdate': '2025-09-01T15:58:49+00:00', 'format': 'PDF 1.4', 'page': 0, 'keywords': 'FlowCV – Online Resume Builder – https://flowcv.com', 'producer': 'Skia/PDF m135', 'creator': 'FlowCV - https://flowcv.com', 'total_pages': 2, 'moddate': '2025-09-01T15:58:49+00:00', 'creationDate': \"D:20250901155849+00'00'\", 'file_path': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf'}, 1 docs found so far\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "PERCENTAGE : 98%\n",
      "Projects\n",
      "2025\n",
      "Healthcare Chatbot for Disease Diagnosis and Guidance\n",
      "Developed an intelligent chatbot that predicts potential diseases based on user sympto"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms \n",
      "and provides personalized health advice Using ANN and Mistral-7B\n",
      "2024\n",
      "Stock Sentiment Analysis using Machine Learning\n",
      "Developed a machine learning model to predict stock market sentiment based on \n",
      "financial news headlines Using Random Forest.\n",
      "2024\n",
      "Doctor Appointment Booking Website in React JS\n",
      "Built a Frontend for Appointment Booking Website using React JS with clean and \n",
      "modern UI design.\n",
      "2024\n",
      "Machine Learning - Based Human Disease Diagnosis System\n",
      "Designed a Predictive disease identification system using Machine Learning, \n",
      "demonstrating expertise in data analysis, algorithm development, and model evaluation.\n",
      "2024\n",
      "Personal Portfolio Website Development\n",
      "Designed and developed a responsive personal portfolio website using HTML, CSS, and \n",
      "JavaScript to showcase projects, skills, and contact information.\n",
      "2023\n",
      "\n",
      "Question: Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: **Year:** 2025\n",
      "**Description:** Developed an intelligent chatbot that predicts potential diseases based on user symptoms and provides personalized health advice.\n",
      "**Technologies Used:** ANN and Mistral-7B.\n",
      "\n",
      "Citations:\n",
      "[1] ..\\data\\pdf\\CHRIS-GEORGE.pdf (page 0)\n",
      "Summary: In 2025, an intelligent chatbot was developed to predict potential diseases based on user symptoms. This chatbot also provides personalized health advice, utilizing ANN and Mistral-7B technologies.\n",
      "History: {'question': 'Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance', 'answer': '**Year:** 2025\\n**Description:** Developed an intelligent chatbot that predicts potential diseases based on user symptoms and provides personalized health advice.\\n**Technologies Used:** ANN and Mistral-7B.', 'sources': [{'source': '..\\\\data\\\\pdf\\\\CHRIS-GEORGE.pdf', 'page': 0, 'score': 0.16064727306365967, 'preview': 'PERCENTAGE : 98%\\nProjects\\n2025\\nHealthcare Chatbot for Disease Diagnosis and Guidance\\nDeveloped an intelligent chatbot th...'}], 'summary': 'In 2025, an intelligent chatbot was developed to predict potential diseases based on user symptoms. This chatbot also provides personalized health advice, utilizing ANN and Mistral-7B technologies.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"Details about the project named Healthcare Chatbot for Disease Diagnosis and Guidance\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413fd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
